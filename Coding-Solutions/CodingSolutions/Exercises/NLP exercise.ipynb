{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import nltk.data\n",
    "import PyPDF2\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_listing(pick):\n",
    "            \n",
    "        stlist = []\n",
    "            \n",
    "        tokenize_it = word_tokenize(basic_text[pick])\n",
    "            \n",
    "        for w in tokenize_it:\n",
    "                \n",
    "            if w in stopwords.words('english'):\n",
    "                    \n",
    "                stlist.append(w)\n",
    "        return stlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is a NLP exercise using THE SORROWS OF YOUNG WERTHER\")\n",
    "\n",
    "# getting site\n",
    "result = requests.get(\"https://www.gutenberg.org/files/2527/2527-h/2527-h.htm\")\n",
    "# makin' it soup\n",
    "soup = bs4.BeautifulSoup(result.text,'lxml')\n",
    "#creating list for texts\n",
    "selected_soup_texts= []\n",
    "\n",
    "#creatin a loop for getting all paragraphs\n",
    "\n",
    "main_story = soup.select('p')[17:]\n",
    "\n",
    "#Listing every paragraph\n",
    "\n",
    "got_text =[]\n",
    "\n",
    "for w in range(len(main_story)):\n",
    "    \n",
    "    mining_text = main_story[w].getText()\n",
    "    got_text.append(mining_text)\n",
    "    \n",
    "\n",
    "#Clean text list\n",
    "basic_text = []\n",
    "\n",
    "for p in got_text:\n",
    "    \n",
    "    first_act = p.replace('\\n',' ')\n",
    "    \n",
    "    second_act = first_act.replace('  ',\"\")\n",
    "    \n",
    "    basic_text.append(second_act)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "        try:\n",
    "            \n",
    "            pick = int(input(\"Please choose a paragraph (0-431): \\n=>\"))\n",
    " \n",
    "        except ValueError:\n",
    "            \n",
    "            print(\"Please choose a number between 0 to 431 \\n=>\")\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        if pick<0 or pick>431:\n",
    "            \n",
    "            print(\"Please choose a number between 0 to 431 \\n=>\")\n",
    "            \n",
    "            continue\n",
    "         \n",
    "        else:\n",
    "            \n",
    "            print(basic_text[pick])\n",
    "            \n",
    "            \n",
    "        \n",
    "        act= input(\"\\n\\n Actions: \\n<> Divide into sentences : dis\\n<> See the stopwords: stw\\n<> Extract the stopwords: estw \\n<> Set of all words: saw \\n<> Words and usage amount: wua\\n=> \\n\\n[[E]xit]\")\n",
    "        \n",
    "        taken_act = act.lower()\n",
    "        \n",
    "       \n",
    "        if  \"dis\"== taken_act :\n",
    "            \n",
    "            sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        \n",
    "            print(('\\n\\n').join(sent_detector.tokenize(basic_text[pick].strip()))) \n",
    "        \n",
    "        \n",
    "            proceed_input = input(\"Do you want to proceed? [y] or [n]\")\n",
    "            \n",
    "            if proceed_input.lower() == \"n\":\n",
    "                break\n",
    "            elif proceed_input.lower() == \"y\":\n",
    "                continue\n",
    "            \n",
    "\n",
    "        elif \"stw\" == taken_act:\n",
    "            \n",
    "            stlist = []\n",
    "            \n",
    "            tokenize_it = word_tokenize(basic_text[pick])\n",
    "            \n",
    "            for w in tokenize_it:\n",
    "                \n",
    "                if w in stopwords.words('english'):\n",
    "                    \n",
    "                    stlist.append(w)\n",
    "                    \n",
    "            print(stlist[:])\n",
    "            \n",
    "            list_inp = input(\"Do you want me to list it for you? [y] or [n]\\n=>\")\n",
    "            \n",
    "            if list_inp.lower() in \" y \":\n",
    "                \n",
    "                print(set(stlist[:]))\n",
    "            \n",
    "            elif list_inp.lower() in \" n \":\n",
    "                \n",
    "                break\n",
    "                \n",
    "            proceed_input = input(\"Do you want to proceed? [y] or [n]\")\n",
    "            \n",
    "            if proceed_input.lower() == \"n\":\n",
    "                break\n",
    "            elif proceed_input.lower() == \"y\":\n",
    "                continue\n",
    "        \n",
    "        elif  \"estw\" ==taken_act: \n",
    "            \n",
    "            picked_p_stw = stopwords_listing(pick)\n",
    "            \n",
    "            tokenize_it = word_tokenize(basic_text[pick])\n",
    "            \n",
    "            for w in tokenize_it:\n",
    "                \n",
    "                if w.lower() in picked_p_stw:\n",
    "                    \n",
    "                    tokenize_it.remove(w)\n",
    "                \n",
    "            print(\" \".join(tokenize_it))\n",
    "            \n",
    "            proceed_input = input(\"Do you want to proceed? [y] or [n]\")\n",
    "        \n",
    "            if proceed_input.lower() == \"n\":\n",
    "                break\n",
    "            elif proceed_input.lower() == \"y\":\n",
    "                continue\n",
    "                \n",
    "        elif taken_act == \"e\":\n",
    "            break\n",
    "\n",
    "        elif taken_act == \"saw\":\n",
    "            \n",
    "            tokenize_it = word_tokenize(basic_text[pick])\n",
    "            \n",
    "            print(set(tokenize_it))\n",
    "            \n",
    "            proceed_input = input(\"Do you want to proceed? [y] or [n]\")\n",
    "        \n",
    "            if proceed_input.lower() == \"n\":\n",
    "                break\n",
    "            elif proceed_input.lower() == \"y\":\n",
    "                continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planning to add:\n",
    "\n",
    "+Words and how many times those words used in text\n",
    "+Continue to process with chosen paragraph or else\n",
    "+Regex for searching purpose ( Experimental way of creating basic search tool)\n",
    "+More nltk library features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
